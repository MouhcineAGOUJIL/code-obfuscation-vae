# üîê Projet d'Obfuscation de Code Python avec VAE

## üìã Description du Projet

Ce projet impl√©mente un syst√®me d'obfuscation de code Python utilisant plusieurs techniques :

- **Renommage de variables** : Transformation des noms de variables en identifiants g√©n√©riques
- **Ajout de code mort** : Insertion de code inutile pour complexifier la lecture
- **Modification d'espaces** : Ajout de commentaires et modification de la structure
- **VAE (Variational Autoencoder)** : G√©n√©ration de variantes de code via apprentissage profond

## üèóÔ∏è Structure du Projet

```
Projet/
‚îú‚îÄ‚îÄ data/                          # Dossier contenant les snippets d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ snippet1.py               # Exemple : fonction add
‚îÇ   ‚îú‚îÄ‚îÄ snippet2.py               # Exemple : fonction factorial
‚îÇ   ‚îî‚îÄ‚îÄ snippet3.py               # Exemple : fonction greet
‚îú‚îÄ‚îÄ model_vae.py                  # Mod√®le VAE (encoder/decoder)
‚îú‚îÄ‚îÄ obfuscate.py                  # Pipeline d'obfuscation
‚îú‚îÄ‚îÄ train.py                      # Script d'entra√Ænement du VAE
‚îú‚îÄ‚îÄ tests.py                      # Tests sans VAE
‚îú‚îÄ‚îÄ tests_with_vae.py             # Tests avec VAE
‚îî‚îÄ‚îÄ README.md                     # Ce fichier
```

## üîß Installation

### Pr√©requis

- Python 3.8 ou sup√©rieur
- pip (gestionnaire de paquets Python)

### √âtape 1 : Installer les d√©pendances

```bash
pip install tensorflow numpy
```

Ou avec un fichier `requirements.txt` :

```bash
# Cr√©er requirements.txt avec :
cat > requirements.txt << EOF
tensorflow>=2.14.0
numpy>=1.24.0
EOF

# Puis installer :
pip install -r requirements.txt
```

### √âtape 2 : V√©rifier l'installation

```bash
python -c "import tensorflow as tf; print('TensorFlow version:', tf.__version__)"
```

## üöÄ Utilisation

### Option 1 : Tests Rapides (Sans VAE)

Si vous voulez tester rapidement les techniques d'obfuscation de base :

```bash
python tests.py
```

**R√©sultat attendu :**

- Affichage de 3 snippets originaux
- Pour chaque snippet : 2-3 variantes obfusqu√©es
- Validation syntaxique de chaque variante

### Option 2 : Utilisation Compl√®te (Avec VAE)

#### √âtape 1 : Entra√Æner le mod√®le VAE

```bash
python train.py
```

**Ce que fait ce script :**

1. Charge tous les fichiers `.py` du dossier `data/`
2. Entra√Æne le VAE pendant 15 epochs
3. Sauvegarde les poids dans :
   - `vae_encoder.weights.h5`
   - `vae_decoder.weights.h5`
   - `vae_tokenizer.json`

**Dur√©e estim√©e :** 1-2 minutes

**R√©sultat attendu :**

```
Attention: dataset petit. Ajoute des snippets dans data/*.py pour un meilleur entra√Ænement.
Epoch 1/15
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15s - kl_loss: 3.28e-05 - loss: 13.66 ...
...
Epoch 15/15
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s - kl_loss: 1.53 - loss: 8.46 ...
Entra√Ænement termin√©. Poids sauvegard√©s (vae_encoder.weights.h5, vae_decoder.weights.h5).
```

#### √âtape 2 : Tester avec le VAE

```bash
python tests_with_vae.py
```

**Ce que fait ce script :**

1. Charge le mod√®le VAE entra√Æn√©
2. Applique les techniques d'obfuscation classiques
3. G√©n√®re des variantes via le VAE
4. Valide la syntaxe de chaque variante

**R√©sultat attendu :**

```
============================================================
TESTS AVEC VAE (Variational Autoencoder)
============================================================

========================================
SNIPPET 1 (original):
def add(a, b):
    return a + b

print(add(2,3))

--- variante 1 ‚úì (validit√© syntaxique: True) ---
def _0(_1, _2):
    return _1 + _2

_3(_0(2,3))

--- variante 2 ‚úì (validit√© syntaxique: True) ---
# Auto-generated code
_unused_var_1 = lambda x: x * 2
_unused_var_2 = [i for i in range(10)]

def add(a, b):
    return a + b

print(add(2,3))

[... autres variantes incluant celles g√©n√©r√©es par le VAE ...]
```

## üìö Utilisation Programmatique

### Dans votre propre code Python

```python
from obfuscate import obfuscate_snippet

# Code original
code = """
def add(a, b):
    return a + b

print(add(2, 3))
"""

# G√©n√©rer des variantes sans VAE
variants_simple = obfuscate_snippet(code, n_variants=2, use_vae=False)

# G√©n√©rer des variantes avec VAE (n√©cessite d'avoir entra√Æn√© le mod√®le)
variants_vae = obfuscate_snippet(code, n_variants=3, use_vae=True)

# Afficher les r√©sultats
for i, variant in enumerate(variants_vae, 1):
    print(f"=== Variante {i} ===")
    print(variant)
    print()
```

## üéØ Am√©liorer le Mod√®le VAE

Pour obtenir de meilleurs r√©sultats avec le VAE :

### 1. Ajouter plus de snippets d'entra√Ænement

Ajoutez vos propres fichiers Python dans le dossier `data/` :

```bash
# Cr√©er de nouveaux snippets
echo "def multiply(x, y):
    return x * y" > data/snippet4.py

echo "def is_even(n):
    return n % 2 == 0" > data/snippet5.py
```

### 2. Ajuster les hyperparam√®tres

Modifiez `train.py` ligne 25 :

```python
# Augmenter le nombre d'epochs
wrapper = VAEWrapper.train_on_texts(
    texts,
    epochs=30,           # Augmenter de 15 √† 30
    batch_size=8,
    latent_dim=32,
    maxlen=256
)
```

### 3. Ajuster la dimension latente

Pour des codes plus complexes :

```python
wrapper = VAEWrapper.train_on_texts(
    texts,
    epochs=15,
    batch_size=8,
    latent_dim=64,      # Augmenter de 32 √† 64
    maxlen=512          # Augmenter si vos codes sont longs
)
```

## üîç Comprendre les R√©sultats

### M√©triques d'entra√Ænement

Pendant l'entra√Ænement, vous verrez 3 m√©triques :

1. **loss (perte totale)** : Devrait diminuer ‚Üí le mod√®le apprend
2. **reconstruction_loss** : Mesure la capacit√© √† reconstruire le code original
3. **kl_loss** : Mesure la r√©gularisation de l'espace latent (devrait augmenter l√©g√®rement)

**Bon entra√Ænement :**

```
Epoch 1/15: loss: 13.66, kl_loss: 0.00003
Epoch 15/15: loss: 8.46, kl_loss: 1.53
```

‚úÖ La perte totale diminue, KL loss augmente mod√©r√©ment

**Mauvais entra√Ænement :**

```
Epoch 1/15: loss: 13.66, kl_loss: 0.00003
Epoch 15/15: loss: 13.50, kl_loss: 0.00005
```

‚ùå Presque aucun changement ‚Üí le mod√®le n'apprend pas

### Qualit√© des variantes

Une bonne variante devrait :

- ‚úÖ √ätre syntaxiquement valide (peut √™tre pars√©e par Python)
- ‚úÖ Ressembler au code original mais avec des modifications
- ‚úÖ Conserver la structure g√©n√©rale du code

Une mauvaise variante :

- ‚ùå Erreurs de syntaxe
- ‚ùå Code compl√®tement illisible ou corrompu
- ‚ùå Perte totale de la structure

## üêõ R√©solution de Probl√®mes

### Probl√®me 1 : Erreur d'importation TensorFlow

```
ImportError: No module named 'tensorflow'
```

**Solution :**

```bash
pip install --upgrade tensorflow
```

### Probl√®me 2 : Mod√®le VAE non trouv√©

```
FileNotFoundError: vae_encoder.weights.h5 not found
```

**Solution :**

```bash
# Entra√Æner d'abord le mod√®le
python train.py
```

### Probl√®me 3 : Dataset trop petit

```
Attention: dataset petit. Ajoute des snippets dans data/*.py
```

**Solution :**
Ajoutez plus de fichiers `.py` dans le dossier `data/`. Le VAE fonctionne mieux avec au moins 10-20 snippets.

### Probl√®me 4 : Variantes VAE invalides syntaxiquement

**Cause :** Le mod√®le n'a pas assez appris (dataset trop petit ou pas assez d'epochs)

**Solution :**

1. Ajouter plus de snippets dans `data/`
2. Augmenter le nombre d'epochs dans `train.py`
3. Relancer l'entra√Ænement

### Probl√®me 5 : Erreurs oneDNN

```
oneDNN custom operations are on...
```

**Ce n'est PAS une erreur** - Ce sont juste des informations. Vous pouvez les ignorer ou les masquer :

```bash
# Sur Windows (PowerShell)
$env:TF_ENABLE_ONEDNN_OPTS="0"
python train.py

# Sur Linux/Mac
export TF_ENABLE_ONEDNN_OPTS=0
python train.py
```

## üìä Architecture du VAE

### Composants principaux

```
Input (code Python)
    ‚Üì
[CharTokenizer] ‚Üí Convertit en s√©quence de nombres
    ‚Üì
[Encoder] ‚Üí LSTM Bidirectionnel ‚Üí Espace latent (z)
    ‚Üì
[Sampling] ‚Üí √âchantillonnage gaussien
    ‚Üì
[Decoder] ‚Üí LSTM ‚Üí S√©quence de tokens
    ‚Üì
Output (code obfusqu√©)
```

### Param√®tres par d√©faut

- **Tokenizer** : Caract√®res ASCII (32-127)
- **Max length** : 256 caract√®res
- **Latent dim** : 32 dimensions
- **Encoder** : Embedding(32) ‚Üí BiLSTM(128)
- **Decoder** : LSTM(128) ‚Üí Dense(vocab_size)

## üéì Concepts Cl√©s

### VAE (Variational Autoencoder)

Un VAE est un r√©seau de neurones qui :

1. **Encode** le code en une repr√©sentation compacte (espace latent)
2. **Sample** un point dans cet espace avec du bruit
3. **Decode** ce point pour g√©n√©rer une variante

### Obfuscation

Transformation du code pour le rendre moins lisible tout en pr√©servant sa fonctionnalit√© :

- **Renommage** : `add` ‚Üí `_0`
- **Code mort** : Ajout de variables inutilis√©es
- **Restructuration** : Modification des espaces/commentaires

## üìù Notes Importantes

‚ö†Ô∏è **Ce projet est √† but p√©dagogique uniquement**

- Ne pas utiliser pour cacher du code malveillant
- Les variantes g√©n√©r√©es peuvent ne pas √™tre fonctionnellement √©quivalentes
- Le VAE n√©cessite un dataset cons√©quent pour de bons r√©sultats

## ü§ù Contribuer

Pour am√©liorer ce projet :

1. Ajoutez plus de techniques d'obfuscation dans `obfuscate.py`
2. Am√©liorez l'architecture du VAE dans `model_vae.py`
3. Cr√©ez des tests plus complets
4. Ajoutez des m√©triques de qualit√© des variantes

## üìÑ Licence

Projet p√©dagogique - Libre d'utilisation pour l'apprentissage

## üë®‚Äçüíª Auteur

Projet de d√©monstration pour l'apprentissage de l'obfuscation de code et des VAE

---

**Derni√®re mise √† jour :** Octobre 2025
**Version :** 1.0.0

## üìä M√©triques et Visualisations

### G√©n√©rer les graphiques d'√©valuation

Apr√®s avoir entra√Æn√© le mod√®le, vous pouvez g√©n√©rer des visualisations d√©taill√©es :

```bash
python visualize_metrics.py
```

### Graphiques g√©n√©r√©s

#### 1. üìà Courbes d'Entra√Ænement

![Training Curves](examples/training_curves.png)

Ce graphique montre l'√©volution pendant l'entra√Ænement :

- **Perte Totale** : Devrait diminuer ‚Üí le mod√®le apprend
- **Reconstruction Loss** : Mesure la qualit√© de reconstruction du code
- **KL Divergence** : R√©gularisation de l'espace latent (augmente l√©g√®rement)
- **Comparaison** : Vue combin√©e des deux pertes

**Interpr√©tation :**

- ‚úÖ **Bon** : Loss totale diminue de ~13 √† ~8-9
- ‚úÖ **Bon** : KL loss augmente progressivement (pas de "collapse")
- ‚ùå **Mauvais** : Loss stagne ou augmente

#### 2. ‚úÖ Qualit√© de l'Obfuscation

![Obfuscation Quality](examples/obfuscation_quality.png)

Trois m√©triques importantes :

- **Validit√© Syntaxique** : % de variantes sans erreur de syntaxe
  - üéØ Objectif : > 80%
- **Nombre de Variantes** : Combien de versions diff√©rentes sont g√©n√©r√©es
- **Niveau de Modification** : Diff√©rence de longueur (proxy pour changements)

**Interpr√©tation :**

- ‚úÖ **Excellent** : 90-100% de validit√© syntaxique
- ‚ö†Ô∏è **Acceptable** : 70-90% de validit√©
- ‚ùå **Probl√®me** : < 70% ‚Üí Le mod√®le a besoin de plus d'entra√Ænement

#### 3. üó∫Ô∏è Espace Latent

![Latent Space](examples/latent_space.png)

Visualisation de l'espace latent appris par le VAE :

- **Distribution des moyennes** : Montre comment les codes sont encod√©s
- **Heatmap** : Repr√©sentations latentes de diff√©rents snippets

**Interpr√©tation :**

- ‚úÖ **Bon** : Distribution proche d'une gaussienne centr√©e
- ‚úÖ **Bon** : Snippets similaires ont des repr√©sentations proches
- ‚ùå **Probl√®me** : Valeurs extr√™mes ou distribution uniforme

### M√©triques Cl√©s

| M√©trique                | Valeur Typique | Interpr√©tation              |
| ----------------------- | -------------- | --------------------------- |
| **Perte Finale**        | 8-10           | Plus c'est bas, mieux c'est |
| **KL Divergence**       | 1.0-2.0        | R√©gularisation active       |
| **Validit√© Syntaxique** | > 80%          | Qualit√© des variantes       |
| **Variantes Uniques**   | 3-5            | Diversit√©                   |

### Pourquoi ces m√©triques ?

Contrairement aux t√¢ches de classification (o√π on utilise pr√©cision/rappel/F1), l'obfuscation de code est une **t√¢che g√©n√©rative**. Les m√©triques appropri√©es sont :

‚ùå **Non pertinent pour ce projet :**

- Pr√©cision / Recall
- Matrice de confusion
- Accuracy
- F1-Score

‚úÖ **Pertinent pour ce projet :**

- Perte de reconstruction (qualit√©)
- KL divergence (r√©gularisation)
- Validit√© syntaxique (correction)
- Diversit√© des variantes (cr√©ativit√©)
- Distance d'√©dition (niveau d'obfuscation)

### Am√©liorer les R√©sultats

Si vos m√©triques ne sont pas satisfaisantes :

1. **Validit√© syntaxique basse (< 70%)** :

   ```bash
   # Augmenter les epochs et la taille du dataset
   # Modifier train.py : epochs=30
   ```

2. **KL divergence trop faible (< 0.1)** :

   ```python
   # Dans model_vae.py, augmenter le poids de KL :
   total_loss = reconstruction_loss + 2.0 * kl_loss  # au lieu de 1.0
   ```

3. **Perte ne diminue pas** :
   - Ajouter plus de snippets dans `data/`
   - Augmenter `latent_dim` √† 64
   - Augmenter la taille du LSTM (128 ‚Üí 256)

### Exporter les M√©triques

Les r√©sultats sont automatiquement sauvegard√©s dans `examples/` :

- `training_curves.png` - Courbes d'entra√Ænement
- `obfuscation_quality.png` - Qualit√© des variantes
- `latent_space.png` - Visualisation de l'espace latent

Ces graphiques sont parfaits pour inclure dans un rapport, une pr√©sentation, ou votre README GitHub !
