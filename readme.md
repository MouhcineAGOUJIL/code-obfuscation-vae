# ğŸ” Projet d'Obfuscation de Code Python avec VAE

## ğŸ“‹ Description du Projet

Ce projet implÃ©mente un systÃ¨me d'obfuscation de code Python utilisant plusieurs techniques :

- **Renommage de variables** : Transformation des noms de variables en identifiants gÃ©nÃ©riques
- **Ajout de code mort** : Insertion de code inutile pour complexifier la lecture
- **Modification d'espaces** : Ajout de commentaires et modification de la structure
- **VAE (Variational Autoencoder)** : GÃ©nÃ©ration de variantes de code via apprentissage profond

## ğŸ—ï¸ Structure du Projet

```
Projet/
â”œâ”€â”€ data/                          # Dossier contenant les snippets d'entraÃ®nement
â”‚   â”œâ”€â”€ snippet1.py               # Exemple : fonction add
â”‚   â”œâ”€â”€ snippet2.py               # Exemple : fonction factorial
â”‚   â””â”€â”€ snippet3.py               # Exemple : fonction greet
â”œâ”€â”€ model_vae.py                  # ModÃ¨le VAE (encoder/decoder)
â”œâ”€â”€ obfuscate.py                  # Pipeline d'obfuscation
â”œâ”€â”€ train.py                      # Script d'entraÃ®nement du VAE
â”œâ”€â”€ tests.py                      # Tests sans VAE
â”œâ”€â”€ tests_with_vae.py             # Tests avec VAE
â””â”€â”€ README.md                     # Ce fichier
```

## ğŸ”§ Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de paquets Python)

### Ã‰tape 1 : Installer les dÃ©pendances

```bash
pip install tensorflow numpy
```

Ou avec un fichier `requirements.txt` :

```bash
# CrÃ©er requirements.txt avec :
cat > requirements.txt << EOF
tensorflow>=2.14.0
numpy>=1.24.0
EOF

# Puis installer :
pip install -r requirements.txt
```

### Ã‰tape 2 : VÃ©rifier l'installation

```bash
python -c "import tensorflow as tf; print('TensorFlow version:', tf.__version__)"
```

## ğŸš€ Utilisation

### Option 1 : Tests Rapides (Sans VAE)

Si vous voulez tester rapidement les techniques d'obfuscation de base :

```bash
python tests.py
```

**RÃ©sultat attendu :**

- Affichage de 3 snippets originaux
- Pour chaque snippet : 2-3 variantes obfusquÃ©es
- Validation syntaxique de chaque variante

### Option 2 : Utilisation ComplÃ¨te (Avec VAE)

#### Ã‰tape 1 : EntraÃ®ner le modÃ¨le VAE

```bash
python train.py
```

**Ce que fait ce script :**

1. Charge tous les fichiers `.py` du dossier `data/`
2. EntraÃ®ne le VAE pendant 15 epochs
3. Sauvegarde les poids dans :
   - `vae_encoder.weights.h5`
   - `vae_decoder.weights.h5`
   - `vae_tokenizer.json`

**DurÃ©e estimÃ©e :** 1-2 minutes

**RÃ©sultat attendu :**

```
Attention: dataset petit. Ajoute des snippets dans data/*.py pour un meilleur entraÃ®nement.
Epoch 1/15
1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15s - kl_loss: 3.28e-05 - loss: 13.66 ...
...
Epoch 15/15
1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s - kl_loss: 1.53 - loss: 8.46 ...
EntraÃ®nement terminÃ©. Poids sauvegardÃ©s (vae_encoder.weights.h5, vae_decoder.weights.h5).
```

#### Ã‰tape 2 : Tester avec le VAE

```bash
python tests_with_vae.py
```

**Ce que fait ce script :**

1. Charge le modÃ¨le VAE entraÃ®nÃ©
2. Applique les techniques d'obfuscation classiques
3. GÃ©nÃ¨re des variantes via le VAE
4. Valide la syntaxe de chaque variante

**RÃ©sultat attendu :**

```
============================================================
TESTS AVEC VAE (Variational Autoencoder)
============================================================

========================================
SNIPPET 1 (original):
def add(a, b):
    return a + b

print(add(2,3))

--- variante 1 âœ“ (validitÃ© syntaxique: True) ---
def _0(_1, _2):
    return _1 + _2

_3(_0(2,3))

--- variante 2 âœ“ (validitÃ© syntaxique: True) ---
# Auto-generated code
_unused_var_1 = lambda x: x * 2
_unused_var_2 = [i for i in range(10)]

def add(a, b):
    return a + b

print(add(2,3))

[... autres variantes incluant celles gÃ©nÃ©rÃ©es par le VAE ...]
```

## ğŸ“š Utilisation Programmatique

### Dans votre propre code Python

```python
from obfuscate import obfuscate_snippet

# Code original
code = """
def add(a, b):
    return a + b

print(add(2, 3))
"""

# GÃ©nÃ©rer des variantes sans VAE
variants_simple = obfuscate_snippet(code, n_variants=2, use_vae=False)

# GÃ©nÃ©rer des variantes avec VAE (nÃ©cessite d'avoir entraÃ®nÃ© le modÃ¨le)
variants_vae = obfuscate_snippet(code, n_variants=3, use_vae=True)

# Afficher les rÃ©sultats
for i, variant in enumerate(variants_vae, 1):
    print(f"=== Variante {i} ===")
    print(variant)
    print()
```

## ğŸ¯ AmÃ©liorer le ModÃ¨le VAE

Pour obtenir de meilleurs rÃ©sultats avec le VAE :

### 1. Ajouter plus de snippets d'entraÃ®nement

Ajoutez vos propres fichiers Python dans le dossier `data/` :

```bash
# CrÃ©er de nouveaux snippets
echo "def multiply(x, y):
    return x * y" > data/snippet4.py

echo "def is_even(n):
    return n % 2 == 0" > data/snippet5.py
```

### 2. Ajuster les hyperparamÃ¨tres

Modifiez `train.py` ligne 25 :

```python
# Augmenter le nombre d'epochs
wrapper = VAEWrapper.train_on_texts(
    texts,
    epochs=30,           # Augmenter de 15 Ã  30
    batch_size=8,
    latent_dim=32,
    maxlen=256
)
```

### 3. Ajuster la dimension latente

Pour des codes plus complexes :

```python
wrapper = VAEWrapper.train_on_texts(
    texts,
    epochs=15,
    batch_size=8,
    latent_dim=64,      # Augmenter de 32 Ã  64
    maxlen=512          # Augmenter si vos codes sont longs
)
```

## ğŸ” Comprendre les RÃ©sultats

### MÃ©triques d'entraÃ®nement

Pendant l'entraÃ®nement, vous verrez 3 mÃ©triques :

1. **loss (perte totale)** : Devrait diminuer â†’ le modÃ¨le apprend
2. **reconstruction_loss** : Mesure la capacitÃ© Ã  reconstruire le code original
3. **kl_loss** : Mesure la rÃ©gularisation de l'espace latent (devrait augmenter lÃ©gÃ¨rement)

**Bon entraÃ®nement :**

```
Epoch 1/15: loss: 13.66, kl_loss: 0.00003
Epoch 15/15: loss: 8.46, kl_loss: 1.53
```

âœ… La perte totale diminue, KL loss augmente modÃ©rÃ©ment

**Mauvais entraÃ®nement :**

```
Epoch 1/15: loss: 13.66, kl_loss: 0.00003
Epoch 15/15: loss: 13.50, kl_loss: 0.00005
```

âŒ Presque aucun changement â†’ le modÃ¨le n'apprend pas

### QualitÃ© des variantes

Une bonne variante devrait :

- âœ… ÃŠtre syntaxiquement valide (peut Ãªtre parsÃ©e par Python)
- âœ… Ressembler au code original mais avec des modifications
- âœ… Conserver la structure gÃ©nÃ©rale du code

Une mauvaise variante :

- âŒ Erreurs de syntaxe
- âŒ Code complÃ¨tement illisible ou corrompu
- âŒ Perte totale de la structure

## ğŸ› RÃ©solution de ProblÃ¨mes

### ProblÃ¨me 1 : Erreur d'importation TensorFlow

```
ImportError: No module named 'tensorflow'
```

**Solution :**

```bash
pip install --upgrade tensorflow
```

### ProblÃ¨me 2 : ModÃ¨le VAE non trouvÃ©

```
FileNotFoundError: vae_encoder.weights.h5 not found
```

**Solution :**

```bash
# EntraÃ®ner d'abord le modÃ¨le
python train.py
```

### ProblÃ¨me 3 : Dataset trop petit

```
Attention: dataset petit. Ajoute des snippets dans data/*.py
```

**Solution :**
Ajoutez plus de fichiers `.py` dans le dossier `data/`. Le VAE fonctionne mieux avec au moins 10-20 snippets.

### ProblÃ¨me 4 : Variantes VAE invalides syntaxiquement

**Cause :** Le modÃ¨le n'a pas assez appris (dataset trop petit ou pas assez d'epochs)

**Solution :**

1. Ajouter plus de snippets dans `data/`
2. Augmenter le nombre d'epochs dans `train.py`
3. Relancer l'entraÃ®nement

### ProblÃ¨me 5 : Erreurs oneDNN

```
oneDNN custom operations are on...
```

**Ce n'est PAS une erreur** - Ce sont juste des informations. Vous pouvez les ignorer ou les masquer :

```bash
# Sur Windows (PowerShell)
$env:TF_ENABLE_ONEDNN_OPTS="0"
python train.py

# Sur Linux/Mac
export TF_ENABLE_ONEDNN_OPTS=0
python train.py
```

## ğŸ“Š Architecture du VAE

### Composants principaux

```
Input (code Python)
    â†“
[CharTokenizer] â†’ Convertit en sÃ©quence de nombres
    â†“
[Encoder] â†’ LSTM Bidirectionnel â†’ Espace latent (z)
    â†“
[Sampling] â†’ Ã‰chantillonnage gaussien
    â†“
[Decoder] â†’ LSTM â†’ SÃ©quence de tokens
    â†“
Output (code obfusquÃ©)
```

### ParamÃ¨tres par dÃ©faut

- **Tokenizer** : CaractÃ¨res ASCII (32-127)
- **Max length** : 256 caractÃ¨res
- **Latent dim** : 32 dimensions
- **Encoder** : Embedding(32) â†’ BiLSTM(128)
- **Decoder** : LSTM(128) â†’ Dense(vocab_size)

## ğŸ“ Concepts ClÃ©s

### VAE (Variational Autoencoder)

Un VAE est un rÃ©seau de neurones qui :

1. **Encode** le code en une reprÃ©sentation compacte (espace latent)
2. **Sample** un point dans cet espace avec du bruit
3. **Decode** ce point pour gÃ©nÃ©rer une variante

### Obfuscation

Transformation du code pour le rendre moins lisible tout en prÃ©servant sa fonctionnalitÃ© :

- **Renommage** : `add` â†’ `_0`
- **Code mort** : Ajout de variables inutilisÃ©es
- **Restructuration** : Modification des espaces/commentaires

## ğŸ“ Notes Importantes

âš ï¸ **Ce projet est Ã  but pÃ©dagogique uniquement**

- Ne pas utiliser pour cacher du code malveillant
- Les variantes gÃ©nÃ©rÃ©es peuvent ne pas Ãªtre fonctionnellement Ã©quivalentes
- Le VAE nÃ©cessite un dataset consÃ©quent pour de bons rÃ©sultats

## ğŸ¤ Contribuer

Pour amÃ©liorer ce projet :

1. Ajoutez plus de techniques d'obfuscation dans `obfuscate.py`
2. AmÃ©liorez l'architecture du VAE dans `model_vae.py`
3. CrÃ©ez des tests plus complets
4. Ajoutez des mÃ©triques de qualitÃ© des variantes

## ğŸ“„ Licence

Projet pÃ©dagogique - Libre d'utilisation pour l'apprentissage

## ğŸ‘¨â€ğŸ’» Auteur

Projet de dÃ©monstration pour l'apprentissage de l'obfuscation de code et des VAE

---

**DerniÃ¨re mise Ã  jour :** Octobre 2025
**Version :** 1.0.0
